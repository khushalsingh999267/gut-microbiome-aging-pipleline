# Gut-microbiomeâ€‘PRJNA931847

A reproducible, beginnerâ€‘friendly Snakemake workflow to download and preprocess raw metagenomic reads for BioProject **PRJNA931847** (gutâ€‘microbiome shotgun data). This repository documents:

* Environment setup (Conda)
* Fetching SRA run accessions
* Downloading and converting `.sra` files to paired FASTQ (â‰¤1â€¯GB RAM)
* Project structure and usage instructions

---

## ðŸ“ Repository Structure

```
gut-microbiome-prjna931847/
â”œâ”€â”€ environment.yml         # Conda environment specification
â”œâ”€â”€ config.yaml             # Project parameters for Snakemake
â”œâ”€â”€ Snakefile               # Snakemake download & conversion workflow
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ srr_ids.txt         # List of SRR IDs for PRJNA931847
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                # Output: paired FASTQ files
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ get_srrs.sh         # Bash script to fetch SRR run list
â””â”€â”€ README.md               # This documentation
```

## ðŸ”§ Prerequisites

* **Git** (for version control)
* **Miniconda** or **Anaconda** (for environment management)
* Internet connection to fetch data from NCBI

## âš™ï¸ Setup

1. **Clone this repository**

   ```bash
   git clone https://github.com/khushalsingh999267/gut-microbiome-prjna931847.git
   cd gut-microbiome-prjna931847
   ```

2. **Create and activate the Conda environment**

   ```bash
   conda env create -f environment.yml
   conda activate gut-microbiome
   ```

3. **Install Snakemake (if not included)**
   Snakemake is declared in `environment.yml`; activation should provide the `snakemake` command.

## ðŸ—‚ï¸ Fetching SRR Accessions

Generate the list of SRR run IDs for BioProject **PRJNA931847**:

```bash
bash scripts/get_srrs.sh
```

* Populates `metadata/srr_ids.txt` (43 runs)
* Easy to re-run if new samples are added

## ðŸš€ Running the Download & Conversion Workflow

This step downloads each `.sra` file, splits into paired FASTQ, and compresses themâ€”all within a 1â€¯GB RAM budget.

```bash
snakemake --use-conda -j 2 --resources mem_mb=1000
```

* `-j 2`: at most two concurrent jobs
* `--resources mem_mb=1000`: caps each ruleâ€™s RAM usage

**Output:** `data/raw/SRRxxxxx_1.fastq.gz` and `SRRxxxxx_2.fastq.gz` for each run.

## ðŸ“ˆ Next Steps

1. **Quality Control & Hostâ€‘read Removal** (`fastp` + `kneaddata`)
2. **Taxonomic Profiling** (MetaPhlAn)
3. **Functional Profiling** (HUMAnN)
4. **Statistical Analysis & Visualization** (R scripts)
5. **Interactive Dashboard** (Streamlit)

We will extend this pipeline stepâ€‘byâ€‘step while maintaining reproducibility and lowâ€‘resource requirements.

## ðŸ“¬ Contributions & Contact

Feel free to open issues or pull requests. For questions, contact:

> Khushal Singh (github.com/ksingh999267)

---

*Version*: 0.1.0 | *Date*: 2025â€‘05â€‘20
