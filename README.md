````markdown

# Metagenome-10Ã— ğŸš€  
*A lightning-fast Snakemake pipeline that auto-downloads any public **BioProject** and processes the **first 10 paired-end samples** end-to-end: **download â†’ QC â†’ taxonomic profiling â†’ reports**.*

Designed for:

* ğŸ”¬ **Rapid pilot studies** â€“ sanity-check datasets before scaling to hundreds of samples.  
* ğŸ“ **Teaching labs & workshops** â€“ a bite-sized workflow that still showcases best practices.  
* ğŸ§ª **Benchmarking** â€“ compare new tools on a consistent, reproducible backbone.

---

## âœ¨ Key features

|  |  |
| --- | --- |
| **`get_srrs.sh` helper** | One-liner that calls the ENA API and writes *exactly* 10 paired-end SRR IDs to `metadata/srr_ids.txt`. |
| **Modular rules** | `prefetch` â†’ `fasterq_dump` â†’ `compress` â†’ `fastp` â†’ `multiqc` â†’ `metaphlan`. |
| **Single-file config** | All tunables (threads, memory, paths, databases) live in `config.yaml`. |
| **Reproducible Conda env** | `environment.yml` pins `snakemake`, `sra-tools`, `fastp`, `kraken2`, `bracken`, `diamond`, etc. |
| **Laptop-to-cluster ready** | Run `snakemake --cores 4` locally or attach an HPC/Cloud profile. |
| **Easy to extend** | Drop in extra rules (Kneaddata host removal, HUMAnN, assembly, binning) without touching the core. |

---

## ğŸš€ Quick start (five lines)

```bash
# 1. Clone
git clone https://github.com/YOURNAME/metagenome-10x.git
cd metagenome-10x

# 2. Reproducible environment
conda env create -f environment.yml
conda activate gut-microbiome

# 3. Grab 10 SRRs from your BioProject (default: PRJEB43097, edit inside get_srrs.sh)
./get_srrs.sh            # writes metadata/srr_ids.txt

# 4. Dry-run & visualize DAG
snakemake -n --dag | dot -Tpng > workflow_dag.png   # requires graphviz

# 5. Launch the pipeline ğŸš€
snakemake --use-conda --cores 4
````

Open **`results/multiqc/multiqc_report.html`** in your browser for an at-a-glance QC summary.

---

## ğŸ—ºï¸ Workflow overview

> Regenerate anytime with `snakemake --dag` (GitHub renders Mermaid).

```mermaid
graph TD
    prefetch["prefetch"] --> fasterq["fasterq_dump"]
    fasterq --> compress["compress (pigz)"]
    compress --> fastp["fastp"]
    fastp --> multiqc["MultiQC"]
    fastp --> metaphlan["MetaPhlAn"]
    multiqc --> all[(all)]
    metaphlan --> all
```

---

## âš™ï¸ Configuration (`config.yaml`)

```yaml
raw_dir: "data/clean"          # final gzipped FASTQs
sra_dir: "data/sra"            # .sra files
fq_dir:  "data/fq"             # intermediate FASTQ
threads: 2                     # default threads per rule
mem_mb: 1000                   # default memory (MB)
kraken_db:  "databases/kraken2db"
diamond_db: "databases/nr"
srr_list_file: "metadata/srr_ids.txt"
```

Change paths, threads, memoryâ€”or point to pre-downloaded databasesâ€”in a single file.

---

## ğŸ“‚ Output structure

```
data/
 â”œâ”€ sra/            # raw .sra files (prefetch)
 â”œâ”€ fq/             # intermediate FASTQ (fasterq_dump)
 â””â”€ clean/          # compressed FASTQ (.gz)

results/
 â”œâ”€ fastp/          # per-sample HTML & JSON QC
 â”œâ”€ multiqc/        # aggregated QC (HTML)
 â””â”€ metaphlan/      # {SRR}_profile.txt
```

Each step is cached; rerunning the pipeline skips completed files automatically.

---

## ğŸ”§ Extending the pipeline

| Goal                          | Add/modify rule(s)                                             |
| ----------------------------- | -------------------------------------------------------------- |
| **Host read removal**         | Insert `kneaddata` between **compress** and **fastp**.         |
| **Kraken2/Bracken profiling** | Branch in parallel to MetaPhlAn after **fastp**.               |
| **Functional annotation**     | DIAMOND + HUMAnN3 downstream of **fastp**.                     |
| **Assembly & binning**        | MEGAHIT â†’ MetaBAT2 â†’ CheckM branch.                            |
| **Cloud scaling**             | Use Snakemake executor profiles (e.g., AWS Batch, Kubernetes). |

---

## ğŸ–‡ï¸ Repository layout

```
.
â”œâ”€â”€ Snakefile               # workflow logic
â”œâ”€â”€ config.yaml             # user-tunable settings
â”œâ”€â”€ environment.yml         # reproducible Conda env
â”œâ”€â”€ get_srrs.sh             # fetch 10 SRR IDs from ENA
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ srr_ids.txt         # auto-generated list of SRRs
â”œâ”€â”€ results/                # pipeline outputs (git-ignored)
â”œâ”€â”€ docs/                   # optional sphinx/mkdocs site
â”œâ”€â”€ tests/                  # tiny 2-sample dataset + CI sanity test
â”œâ”€â”€ .github/workflows/ci.yml# CI: lint + dry-run each PR
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md               # â† you are here
```

---

## ğŸ¤ Contributing

1. **Fork**, create a feature branch, code away.
2. Run `snakemake --lint` and `snakemake -n` (dry-run) â€“ CI must stay green.
3. Submit a pull request describing *why* the change helps end-users.

---

## ğŸ“œ License

Distributed under the **GPL-3.0** License â€“ see `LICENSE` for full text.

---

## ğŸ“– Citation

If this pipeline supports your research, please cite via [`CITATION.cff`](CITATION.cff) or the Zenodo DOI on the GitHub release page.

> *â€œStart small, iterate fast, scale wisely.â€* â€“ **Metagenome-10Ã—** team
> Happy metagenoming! ğŸ‰

```


